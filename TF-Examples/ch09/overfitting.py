# 过拟合
import tensorflow as tf

# 1.模型的容量
"""
通俗的讲，模型的容量或表达能力，是指模型拟合复杂函数的能力。一种体现模型容
量的指标为模型的假设空间(Hypothesis Space)大小，即模型可以表示的函数集的大小。假
设空间越大越完备，从假设空间中搜索出逼近真实模型的函数也就越有可能；反之，如果
假设空间非常受限，就很难从中找到逼近真实模型的函数。
"""

# 2.过拟合与欠拟合
"""
当模型的容量过大时，网络模型除了学习到训练集数据的模态之外，还把额外的观测
误差也学习进来，导致学习的模型在训练集上面表现较好，但是在未见的样本上表现不
佳，也就是泛化能力偏弱，我们把这种现象叫做过拟合(Overfitting)。当模型的容量过小
时，模型不能够很好的学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的
样本上表现也不佳，我们把这种现象叫做欠拟合(Underfitting)。
"""
"""
那么如何去选择模型的容量？统计学习理论给我们提供了一些思路，其中VC 维度
(Vapnik-Chervonenkis 维度)是一个应用比较广泛的度量函数容量的方法。尽管这些方法给
机器学习提供了一定程度的理论保证，但是这些方法却很少应用到深度学习中去，一部分
原因是神经网络过于复杂，很难去确定网络结构背后的数学模型的VC 维度。
"""
"""
尽管统计学习理论很难给出神经网络所需要的最小容量，但是我们却可以根据奥卡姆
剃刀原理(Occam’s razor)来指导神经网络的设计和训练。奥卡姆剃刀原理是由14 世纪逻辑
学家、圣方济各会修士奥卡姆的威廉(William of Occam)提出的一个解决问题的法则，他在
《箴言书注》2 卷15 题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事
情’。”。也就是说，如果两层的神经网络结构能够很好的表达真实模型，那么三层的神经
网络也能够很好的表达，但是我们应该优先选择使用更简单的两层神经网络，因为它的参
数量更少，更容易训练、更容易通过较少的训练样本获得不错的泛化误差。
"""


# 3.数据集划分
"""
前面我们介绍了数据集需要划分为训练集(Train set)和测试集(Test set)，但是为了挑选模型超参数和检测过拟合现象，
一般需要将原来的训练集再次切分为新的训练集和验证集(Validation set)，即数据集需要切分为训练集、验证集和测试集3 个子集。
"""


# 4.模型设计
"""
通过验证集可以判断网络模型是否过拟合或者欠拟合，从而为调整网络模型的容量提
供判断依据。对于神经网络来说，网络的层数和参数量是网络容量很重要的参考指标，通
过减少网络的层数，减少每层中网络参数量的规模可以有效降低网络的容量。反之，如果
发现模型欠拟合，需要增大网络的容量，可以通过增加层数，增大每层的参数量等方式实现。
"""


# 5.正则化
# L0 正则化
# L1 正则化
"""
定义为张量𝜃𝑖中所有元素的绝对值之和。L1 正则化也叫Lasso Regularization，它是连续可导的，在神经网络中使用广泛
"""
# 创建网络参数w1,w2
w1 = tf.random.normal([4, 3])
w2 = tf.random.normal([4, 2])
# 计算L1 正则化项
loss_reg = tf.reduce_sum(tf.math.abs(w1)) + tf.reduce_sum(tf.math.abs(w2))
# L2 正则化
"""
定义为张量𝜃𝑖中所有元素的平方和。L2 正则化也叫Ridge Regularization，它和L1 正则化一样，也是连续可导的，在神经网络中使用广泛。
"""
# 计算L2 正则化项
loss_reg = tf.reduce_sum(tf.square(w1)) + tf.reduce_sum(tf.square(w2))


# 5.Dropout
"""
Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；但是在测试时，Dropout 会恢复所有的连接，保证模型测试时获得最好的性能。
"""
# 在 TensorFlow 中，可以通过tf.nn.dropout(x, rate)函数实现某条连接的Dropout 功能，
# 其中rate 参数设置断开的概率值𝑝：
# 添加dropout 操作
# x = tf.nn.dropout(x, rate=0.5)
# 也可以将Dropout 作为一个网络层使用，在网络中间插入一个Dropout 层：
# 添加Dropout 层
# model.add(layers.Dropout(rate=0.5))