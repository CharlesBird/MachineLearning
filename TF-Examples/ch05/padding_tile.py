# å¼ é‡å¡«å……ä¸å¤åˆ¶
import tensorflow as tf

# å¡«å……
"""
é€šè¿‡ tf.pad(x, paddings)å‡½æ•°å®ç°ï¼Œpaddings æ˜¯åŒ…å«äº†å¤šä¸ª[ğ¿ğ‘’ğ‘“ğ‘¡ ğ‘ƒğ‘ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”, ğ‘…ğ‘–ğ‘”â„ğ‘¡ ğ‘ƒğ‘ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”]çš„åµŒå¥—æ–¹æ¡ˆ Listï¼Œ
å¦‚[[0,0],[2,1],[1,2]]è¡¨ç¤ºç¬¬ä¸€ä¸ªç»´åº¦ä¸å¡«å……ï¼Œç¬¬äºŒä¸ªç»´åº¦å·¦è¾¹(èµ·å§‹å¤„)å¡«å……ä¸¤ä¸ªå•å…ƒï¼Œå³è¾¹(ç»“æŸå¤„)å¡«å……ä¸€ä¸ªå•å…ƒï¼Œ
ç¬¬ä¸‰ä¸ªç»´åº¦å·¦è¾¹å¡«å……ä¸€ä¸ªå•å…ƒï¼Œå³è¾¹å¡«å……ä¸¤ä¸ªå•å…ƒ
"""
a = tf.constant([1, 2, 3, 4, 5, 6])
b = tf.constant([7, 8, 1, 6])
b = tf.pad(b, [[0, 2]])  # å¡«å……
print(b)

# å¡«å……åå¥å­å¼ é‡å½¢çŠ¶ä¸€è‡´ï¼Œå†å°†è¿™ 2 å¥å­ Stack åœ¨ä¸€èµ·
print(tf.stack([a, b], axis=0))

total_words = 10000  # è®¾å®šè¯æ±‡é‡å¤§å°
max_review_len = 80  # æœ€å¤§å¥å­é•¿åº¦
embedding_len = 100  # è¯å‘é‡é•¿åº¦
# åŠ è½½ IMDB æ•°æ®é›†
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=total_words)
# å°†å¥å­å¡«å……æˆ–æˆªæ–­åˆ°ç›¸åŒé•¿åº¦ï¼Œè®¾ç½®ä¸ºæœ«å°¾å¡«å……å’Œæœ«å°¾æˆªæ–­æ–¹å¼
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len, truncating='post', padding='post')
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len, truncating='post', padding='post')
print(x_train.shape, x_test.shape)


"""
è€ƒè™‘å¯¹å›¾ç‰‡çš„é«˜å®½ç»´åº¦è¿›è¡Œå¡«å……ã€‚ä»¥ 28x28 å¤§å°çš„å›¾ç‰‡æ•°æ®ä¸ºä¾‹ï¼Œå¦‚æœç½‘ç»œå±‚æ‰€æ¥å—çš„æ•°æ®é«˜å®½ä¸º 32x32ï¼Œ
åˆ™å¿…é¡»å°† 28x28 å¤§å°å¡«å……åˆ°32x32ï¼Œå¯ä»¥åœ¨ä¸Šã€ä¸‹ã€å·¦ã€å³æ–¹å‘å„å¡«å…… 2 ä¸ªå•å…ƒ
"""
x = tf.random.normal([4, 28, 28, 1])
# å›¾ç‰‡ä¸Šä¸‹ã€å·¦å³å„å¡«å…… 2 ä¸ªå•å…ƒ
print(tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]]))


# å¤åˆ¶
"""
é€šè¿‡ tf.tile å‡½æ•°å¯ä»¥åœ¨ä»»æ„ç»´åº¦å°†æ•°æ®é‡å¤å¤åˆ¶å¤šä»½ï¼Œå¦‚ shape ä¸º[4,32,32,3]çš„æ•°æ®ï¼Œ
å¤åˆ¶æ–¹æ¡ˆ multiples=[2,3,3,1]ï¼Œå³é€šé“æ•°æ®ä¸å¤åˆ¶ï¼Œé«˜å®½æ–¹å‘åˆ†åˆ«å¤åˆ¶ 2 ä»½ï¼Œå›¾ç‰‡æ•°å†å¤åˆ¶
1 ä»½
"""
x = tf.random.normal([4, 32, 32, 3])
print(tf.tile(x, [2, 3, 3, 1]))  # æ•°æ®å¤åˆ¶