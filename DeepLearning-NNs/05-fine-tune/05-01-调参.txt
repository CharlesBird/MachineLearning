卷积神经网络调参
  - 更多优化算法
  - 激活函数
  - 网络初始化
  - 批归一化
  - 数据增强
 
 AdaGrad 算法
  - 调整学习率
  

学习率自适应
 - 对于稀疏数据，使用学习率可适应方法。比如: 百度谷歌，广告点击率，维度特征上亿，只有几百有值
 - SGD通常训练时间更长，最终效果比较好，但需要好的初始化和 learning rate。
 - 需要训练较深较复杂的网络且需要快速收敛时，推荐使用adam。
 
激活函数
sigmoid
 - 输入非常大或者非常小时没有梯度
 - 输出均值非0，对神经网络学习是不友好的
 - Exp 计算复杂
 - 梯度消失，梯度比较深的网络，底层的梯度得不到更新

Tanh
 - 依旧没有梯度
 - 输出均值为0
 - 计算复杂

ReLU
 - 不饱和（梯度不会过小）
 - 计算量小
 - 收敛速度快
 - 输出均值非0
 - Dead ReLU
  - 一个非常大的梯度流过神经元，不会再数据有激活现象了

Leaky-ReLU ，当x小于0时，*一个小于0的系数
 - 解决 Dead ReLU 问题

ELU ， 小于0的部分，用指数的方式进行计算
 - 均值更接近0
 - 小于0的计算量大
 
Maxout ， 有两个 w,b 参数，取最大值
 - ReLU的泛化版本
 - 解决 Dead ReLU 问题
 - 参数 double


设置激活函数技巧
 - ReLU 小心设置learning rate，大于0时导数比较大，最好设置一个比较小的值
 - 不要使用sigmoid，计算太慢
 - 推荐使用Leaky-ReLU, ELU, Maxout， ReLU有缺点
 - tanh，不要报太大的期望，因为计算比较大


网络初始化
如何分析初始化结果好不好？
 - 查看初始化各层的激活值的分布
 - 均值为0，方差为0.02的正态分布初始化
  - tanh, ReLU激活函数


批归一化
 - 每个batch在每一层都做归一化
 - 为了确保归一化能够起作用，另设两个参数来逆归一化， 原因：详情看 5-2，22:00


数据增强
 - 归一化
 - 图像变换
  - 反转，拉伸，裁剪，变形（在数据量不多的情况下）
 - 色彩变换
  - 对比度，亮度
 - 多尺度

更多调参技巧
 - 更多数据量
 - 给神经网络添加层次
 - 紧跟最新进展，使用新方法
 - 增大训练迭代次数
 - 尝试正则化项
 - 使用更多的GPU来加速训练

可视化工具来检查中间状态， 举例图示说明见5-3，10:00
 - 损失
 - 梯度
 - 准确率
 - 学习率

在标准数据集上训练
在小数据集上过拟合
数据集分布平衡
使用预调整好的稳定模型结构
Fine-tuning
 - 预训练好的模型进行微调